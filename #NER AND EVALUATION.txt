#NER AND EVALUATION

import spacy #Import spaCy (for Named Entity Recognition â€” NER).
from sklearn.metrics import precision_score, recall_score, f1_score

# Load the spaCy model
nlp = spacy.load("en_core_web_sm") #Load spaCy's small English model (already trained for NER, POS tagging, etc.)

# Sample text for NER
text = "Ram Raje was born in Hawaii."

# Ground truth (true labels for the entities)
true_entities = [
    ('Barack Obama', 'PERSON'),
    ('Hawaii', 'GPE')
]
+
# Process the text with spaCy to extract named entities
doc = nlp(text)

# Predicted entities (from the NER model)
predicted_entities = [(ent.text, ent.label_) for ent in doc.ents]

# Display predicted entities
print("Predicted Entities:")
for ent in predicted_entities:
    print(f"{ent[0]} - {ent[1]}")

# Evaluate performance using precision, recall, and F1-score
# Extract the labels (names of the entities) for comparison
true_labels = [label for _, label in true_entities]
pred_labels = [label for _, label in predicted_entities]

# Calculate Precision, Recall, and F1-Score
precision = precision_score(true_labels, pred_labels, average='macro', zero_division=1)
recall = recall_score(true_labels, pred_labels, average='macro', zero_division=1)
f1 = f1_score(true_labels, pred_labels, average='macro', zero_division=1)

print("\nEvaluation Metrics:")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-Score: {f1:.2f}")


# Precision =
#Out of all the things you predicted as correct, how many were actually correct?
#"When I say it's good, am I usually right?"

# Recall =
#Out of all the things that were actually correct, how many did you find?
# "Did I find all the good things?"

# F1-Score =
# "Am I good at both being correct and finding everything?"